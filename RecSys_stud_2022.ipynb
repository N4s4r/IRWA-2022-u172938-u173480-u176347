{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVw16koYkxBv"
   },
   "source": [
    "# Recommender Systems - A practical Overview\n",
    "\n",
    "Today we play with some of the models we saw in the last lesson, in particular we're going to:\n",
    "\n",
    "1. Play with user-user and item-item *Memory-based Collaborative Filtering* (CF) models, testing and comparing the two, to see which one gives the best performance\n",
    "\n",
    "2. We test different similarity functions to see which one gives us the highest accuracy\n",
    "\n",
    "3. Play with *Model Based Matrix Factorization* (CF) algorithm, comparing its performance with the models seen before\n",
    "\n",
    "4. Check for popularity bias in the outuput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8DDG8zUkxB4"
   },
   "source": [
    "# 1. Memory-based Collaborative Filtering\n",
    "\n",
    "**How it works**\n",
    "Collaborative Filtering recommender systems are among the most prominent approaches to generate recommendations. They are popular and well-studied in academia but also widely used by industry, likd commercial e-commerce sites. They are based on the concept of \"wisdom of the crowd\", where predictions are generated looking at patterns of similarity in the data. The crucial assumption is *Customers who had similar tastes in the past, will have\n",
    "similar tastes in the future*.\n",
    "\n",
    "**Item-based and User-based**\n",
    "Similarities are extracted in two different ways, looking either at user-user interactions or at item-item interactions. \n",
    "\n",
    "![Figure 1](https://drive.google.com/uc?export=view&id=1R-mvnKgjT2xg0C-hPcnLQqShPnkipRo2)\n",
    "<center><caption> <u>Figure 1</u>: CF</caption></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "szz6qT1NkxB5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bA9JfMCHkxB7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josep Alet\\AppData\\Local\\Temp\\ipykernel_15752\\1916806093.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_ratings = pd.read_csv(\"data/ratings.dat\", sep=\"::\", header = None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first of all we load the data and rename columns\n",
    "df_ratings = pd.read_csv(\"data/ratings.dat\", sep=\"::\", header = None)\n",
    "df_ratings.columns = [\"UserID\",\"MovieID\", \"Rating\", \"Timestamp\"]\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "egSkg3vqkxB8"
   },
   "outputs": [],
   "source": [
    "# we split the dataset in train and test, keeping the 90% as training, the rest as test-set\n",
    "df_test = df_ratings.sample(frac=0.1, random_state=123)\n",
    "df_train = df_ratings[df_ratings.index.isin(df_test.index) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ew7-ZkbVkxB8"
   },
   "outputs": [],
   "source": [
    "# we extract the max-id for users and items\n",
    "n_users = max(df_ratings[\"UserID\"].unique())+1\n",
    "m_items = max(df_ratings[\"MovieID\"].unique())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G2pYxc5kxB9"
   },
   "source": [
    "We then generate the sparse matrix of interactions $M$, where each value $m_{ui}$ corresponds to the rating of the user *u* for the movie *i*.\n",
    "\n",
    "**Why sparse matrix** To handle huge matrices and avoid using memory for zero-entries, we can exploit sparse data structure. If most of the elements are nonzero, then the matrix is considered *dense*. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-rLZ7FQskxB-"
   },
   "outputs": [],
   "source": [
    "def get_sparse_martrix(interactions, weights, n_users, m_items):\n",
    "    interactions = list(zip(*interactions))\n",
    "    adjacency_matrix = csr_matrix((weights, interactions), shape = (n_users,m_items))\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XezyC6qQkxCA"
   },
   "outputs": [],
   "source": [
    "lst_interactions = df_train[[\"UserID\", \"MovieID\"]].values\n",
    "ratings = df_train[\"Rating\"]\n",
    "\n",
    "# generate sparse matrix of ratings\n",
    "M = get_sparse_martrix(lst_interactions, ratings, n_users, m_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kUFvXydkxCB"
   },
   "source": [
    "We then use the cosine similarity score to generate the user-user and item-item pair-wise similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dHbZfbyMkxCB"
   },
   "outputs": [],
   "source": [
    "# user-user similarity\n",
    "user_user_sim = cosine_similarity(M)\n",
    "\n",
    "# item-item similarity\n",
    "item_item_sim = cosine_similarity(M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V-H_5fk5kxCC"
   },
   "outputs": [],
   "source": [
    "def get_baseline_score(M, user_id, item_id):\n",
    "    \"\"\"\n",
    "    it computes the baseline score b_ui = \\mu + b_i + b_u\n",
    "    \n",
    "    \\mu = avg. score given by the dataset\n",
    "    \n",
    "    b_i = avg. score given by all the users to the item i\n",
    "    \n",
    "    b_u = avg. score given by the user u to all the rated elements\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    mean_ = M[M!=0].mean()### your code ###\n",
    "    \n",
    "    b_i = M[:, item_id].data.mean()### your code ### \n",
    "    b_u = M[user_id, :].data.mean()### your code ### \n",
    "    \n",
    "    out = mean_ + b_i + b_u\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE6DZGMMkxCD"
   },
   "outputs": [],
   "source": [
    "def predict_item_item(M, similarity_M, b_ui, user_id, item_id, k = 10):\n",
    "    \"\"\"\n",
    "    \n",
    "    here we compute the final score based on item-item CF model\n",
    "    \n",
    "    \\hat{r}_{ui} = \\frac{\\sum_{j \\in N_k} s(i,j) (r_{uj} - b_{uj}) } {\\sum_{j \\in N_k} s(i,j)}    \n",
    "    \n",
    "    to choose the k to get the N_k we just order the idx and select the first k idxs\n",
    "    \n",
    "    \"\"\"\n",
    "    # similar items\n",
    "    # here extract the idxs of the items which are similar to the selected *item-id*\n",
    "    similar_items_idx = ### your code ###\n",
    "    \n",
    "    # rated items - here we take the column-indexes\n",
    "    # here we extract the idxs of the items rated by the selected user-id\n",
    "    rated_by_user_idx = ### your code ###\n",
    "    \n",
    "    # rated and similar items\n",
    "    # here we find the intersection between the two sets, rated_by_user and similar_items\n",
    "    rated_and_similar = set(similar_items_idx).intersection(rated_by_user_idx)\n",
    "    \n",
    "    # we then consider for our task the first k idxs (ordered by id, *NOT* by score)\n",
    "    rated_and_similar = sorted(rated_and_similar)[:k]\n",
    "\n",
    "    \n",
    "    out = 0\n",
    "    den = 0\n",
    "    \n",
    "    for item_sim_and_rated in rated_and_similar:\n",
    "        \n",
    "        # baseline score for item_sim_and_rated and user_id\n",
    "        one_b_ui = get_baseline_score(### your code ###)\n",
    "        \n",
    "        # similarity between item_sim_and_rated and item_id\n",
    "        s_ij = similarity_M[### your code ###]\n",
    "        \n",
    "        # difference between rating given gy the user_id to item_sim_and_rated AND one_b_ui\n",
    "        diff_ = M[### your code ###] - ### your code ###\n",
    "        \n",
    "        # update num\n",
    "        out+= s_ij*diff_\n",
    "\n",
    "        # update den\n",
    "        den += s_ij\n",
    "    \n",
    "    # compute ratio\n",
    "    out = out/den\n",
    "    \n",
    "    # add b_ui\n",
    "    out = b_ui + out \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhIa98JekxCE"
   },
   "outputs": [],
   "source": [
    "def predict_user_user(similarity_M, b_ui, user_id, item_id, k = 10):\n",
    " \n",
    "    \"\"\"\n",
    "    here we compute the final score based on item-item CF model\n",
    "    \n",
    "    \\hat{r}_{ui} = \\frac{\\sum_{v \\in N_k} s(u,v) (r_{vi} - b_{vi}) } {\\sum_{v \\in N_k} s(u,v)}    \n",
    "    \n",
    "    to choose the k to get the N_k we just order the idx and select the first k idxs\n",
    "    \"\"\"\n",
    "    # similar users\n",
    "    similar_users_idx = ### your code ###\n",
    "    \n",
    "    # rated items - here we take the row-indexes\n",
    "    same_item_rated = ### your code ###\n",
    "    \n",
    "    # rated and similar items\n",
    "    rated_same_and_similar = ### your code ###\n",
    "    \n",
    "    # take the first k\n",
    "    rated_same_and_similar = ### your code ###\n",
    "    \n",
    "    out = 0\n",
    "    den = 0\n",
    "    \n",
    "    for user_same_and_similar in rated_same_and_similar:\n",
    "        \n",
    "        # baseline score for item_sim_and_rated and user_id\n",
    "        one_b_ui = ### your code ###\n",
    "        \n",
    "        # similarity between user_id and user_same_and_similar\n",
    "        s_ij = ### your code ###\n",
    "        \n",
    "        # difference between rating given gy user_same_and_similar AND one_b_ui\n",
    "        diff_ = ### your code ###\n",
    "        \n",
    "        out+= s_ij*diff_\n",
    "\n",
    "        den += s_ij\n",
    "\n",
    "    out = out/den\n",
    "    out = b_ui + out \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNKDcIMNkxCF"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr2qvqgukxCF"
   },
   "outputs": [],
   "source": [
    "# compute the estimate with the two approaches\n",
    "user_id = 2233\n",
    "item_id = 440\n",
    "b_ui = get_baseline_score(M, user_id, item_id)\n",
    "\n",
    "prediction_item_item = predict_item_item(item_item_sim, b_ui, user_id, item_id)\n",
    "prediction_user_user = predict_user_user(user_user_sim, b_ui, user_id, item_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIRBDSDlkxCH"
   },
   "source": [
    "Test the score estimatation for one tuple (uid, iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0kenzO0kxCI"
   },
   "outputs": [],
   "source": [
    "prediction_item_item, prediction_user_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZuuG_cWkxCI"
   },
   "source": [
    "(3.7511541341110797, 3.4264185601314714)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4bXs-bhkxCJ"
   },
   "source": [
    "# 2. Comparison with difference similarity functions\n",
    "\n",
    "Here we test the 2 models presented before with two different functions of similarity. We use *Cosine Similarity* and +Pearson Coefficient* with the item-item approach, testing the results over the first 20 ratings. For the evaluation we use the *RMSE*\n",
    "\n",
    "![Figure 2](https://drive.google.com/uc?export=view&id=1qoAdyepkGu8otUXyAFL2qD9Y8mYPkbMY)\n",
    "<center><caption> <u>Figure 2</u>: Comparison with difference similarity functions</caption></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwiYfpYSkxCJ"
   },
   "outputs": [],
   "source": [
    "# user-user similarity\n",
    "user_user_sim2 = np.corrcoef(M.toarray())\n",
    "\n",
    "# item-item similarity\n",
    "item_item_sim2 = np.corrcoef(M.toarray().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smzlc0p1kxCK"
   },
   "source": [
    "Compare RMSE between the 4 models - compute the rating for the first 10 obs of the test, then compute the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWcnh_yukxCK"
   },
   "outputs": [],
   "source": [
    "def compute_RMSE(list_of_tuples_r_ui_r_ui_hat):\n",
    "    \"\"\"\n",
    "    sqrt(\\sum (r_ui - r_ui_hat)^2 )\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we have the (r_{ui} - \\hat r_{ui})**2 \n",
    "    diff_ = [### your code ### for (r_ui, r_ui_hat) in list_of_tuples_r_ui_r_ui_hat]\n",
    "    \n",
    "    # we then sum the vector of squared differences\n",
    "    out = ### your code ###\n",
    "    \n",
    "    # and then apply the square-root\n",
    "    out = ### your code ###\n",
    "    \n",
    "    # we then divide the final value by the length of the vector - to get an avg. estimate\n",
    "    out = ### your code ###\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7tLowvukxCL"
   },
   "outputs": [],
   "source": [
    "# performance over the top-20 ratings of the test-set\n",
    "\n",
    "r_ui_r_ui_hat_1 = []\n",
    "r_ui_r_ui_hat_2 = []\n",
    "\n",
    "for user_id, item_id, rating in df_test.head(20)[[\"UserID\", \"MovieID\", \"Rating\"]].values:\n",
    "\n",
    "    # compute first the baseline score\n",
    "    b_ui = get_baseline_score(M, user_id, item_id)\n",
    "    \n",
    "    # prediction with cosine similarity \n",
    "    prediction_item_item1 = predict_item_item(item_item_sim, b_ui, user_id, item_id)\n",
    "    r_ui_r_ui_hat_1.append((prediction_item_item1, rating))\n",
    "\n",
    "    # prediction with pearson correlation    \n",
    "    prediction_item_item2 = predict_item_item(item_item_sim2, b_ui, user_id, item_id)    \n",
    "    r_ui_r_ui_hat_2.append((prediction_item_item2, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST1jyg4WkxCL"
   },
   "outputs": [],
   "source": [
    "compute_RMSE(r_ui_r_ui_hat_1), compute_RMSE(r_ui_r_ui_hat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzv2OCIokxCM"
   },
   "source": [
    "(0.1680177565497746, 0.17166301445439586)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtyVKf9kxCM"
   },
   "source": [
    "# 3. Model-based Collaborative Filtering\n",
    "\n",
    "**Matrix Factorization**\n",
    "Matrix factorization models generate low dimensional latent features, for both users and items, in order to catch the relationship between items’ and users’ with respec to the whole dataset. \n",
    "\n",
    "\n",
    "**Library used here**\n",
    "\n",
    "Surprise is a Python scikit library for building and analyzing recommender systems that deal with explicit rating data. It provides tools to evaluate, analyse and compare the algorithms’ performance.\n",
    "\n",
    "[link](http://surpriselib.com/) to the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rth_rkcWkxCM"
   },
   "source": [
    "We now compare the performances of the previous models and 2 other matrix factorization algorithms (SVD and NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5MmSVofkxCN"
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNBaseline\n",
    "from surprise import NMF\n",
    "from surprise import BaselineOnly\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Dataset\n",
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxI6TONdkxCN"
   },
   "outputs": [],
   "source": [
    "# use surprise functions\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ratings[[\"UserID\", \"MovieID\", \"Rating\"]], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIETQsOvkxCO"
   },
   "outputs": [],
   "source": [
    "## svd model\n",
    "svd_model = SVD(n_factors=10, n_epochs=10)\n",
    "svd_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZxlNk80kxCO"
   },
   "outputs": [],
   "source": [
    "## non-negative matrix factorization\n",
    "nmf_model = NMF(n_factors=15, n_epochs=10)\n",
    "nmf_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLU0fcXmkxCO"
   },
   "outputs": [],
   "source": [
    "## user-user CF\n",
    "user_user_CF = KNNBaseline(k=5, sim_options={'user_based': True, 'name': 'pearson_baseline'})\n",
    "user_user_CF.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iSOSGwrkxCO"
   },
   "outputs": [],
   "source": [
    "## item-item CF\n",
    "item_item_CF = KNNBaseline(k=5, sim_options={'user_based': False, 'name': 'pearson_baseline'})\n",
    "item_item_CF.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIsUU3dikxCP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we then test the models and check which gives best performances\n",
    "main_df = pd.DataFrame(columns=[\"Model\", \"RMSE\"])\n",
    "\n",
    "for label, model in [(\"svd\", svd_model), (\"NMF\",nmf_model), (\"UserUser\", user_user_CF), (\"ItemItem\",item_item_CF)]:\n",
    "    predictions_ = model.test(testset)\n",
    "    \n",
    "    # we then extract only the items in position 2 and 3 in each tuple, which are the rating and its estimate\n",
    "    only_scores = ### your code ###\n",
    "    one_rmse = ### your code ###\n",
    "    \n",
    "    main_df.loc[len(main_df)] = ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iEtELgakxCP"
   },
   "outputs": [],
   "source": [
    "# we check here the final result\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYqOumugkxCQ"
   },
   "source": [
    "# 4. Popularity bias\n",
    "\n",
    "Recommender systems are known to suffer from the popularity bias problem: popular (i.e. frequently rated) items get a lot of exposure while less popular ones are under-represented when generating the recommendations. Here we analyze the popularity-bias stimulated by the matrix factorization algorithm, analyzing the frequency of items distribution given by recommendation lists generated for the first 100 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nOkNdfukxCQ"
   },
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = {}\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        # check if user not in the top_n\n",
    "        ### your code ###\n",
    "            ### your code ###\n",
    "        # then add the tuple (iid, est)\n",
    "        ### your code ###\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        # sort by the user-rating the list\n",
    "        ### your code ###\n",
    "        # take only the top-n ratings\n",
    "        ### your code ###\n",
    "        \n",
    "        # take only the iid and not the rating from each tuple\n",
    "        top_n[uid] = ### your code ###\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd0LnWNrkxCQ"
   },
   "source": [
    "We first extract all the missing ratings for the first 100 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04d1VSv0kxCR"
   },
   "outputs": [],
   "source": [
    "missing_ratings = set()\n",
    "all_items = set(trainset.all_items())\n",
    "for uid in range(100):\n",
    "    known_ratings = set([x for x,y in trainset.ur[uid]])\n",
    "    ### your code ###\n",
    "        ### your code ###\n",
    "            missing_ratings.add((uid, iid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ETaB4yIkxCR"
   },
   "source": [
    "Then we generate the prediction with the SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhQnD6PFkxCS"
   },
   "outputs": [],
   "source": [
    "new_predictions = []\n",
    "for uid, iid in missing_ratings:\n",
    "    new_predictions.append(svd_model.predict(uid, iid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nABAWL71kxCS"
   },
   "source": [
    "Then we generate the top-n recommendations for each user, with n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctTs5LTjkxCS"
   },
   "outputs": [],
   "source": [
    "top_n = get_top_n(new_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04cHVQYSkxCT"
   },
   "outputs": [],
   "source": [
    "freq_items = {}\n",
    "for uid in top_n:\n",
    "    new_rec = top_n[uid]\n",
    "    for iid in new_rec:\n",
    "        ### your code ###\n",
    "            ### your code ###\n",
    "        ### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTiP4kuAkxCT"
   },
   "source": [
    "We then plot the distribution of the 100 most frequent items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpQycatlkxCT"
   },
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame.from_records(list(freq_items.items()))\n",
    "df_freq.columns = [\"item\", \"freq\"]\n",
    "df_freq.set_index(\"item\", inplace=True)\n",
    "df_freq.sort_values(\"freq\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC1XF4AMkxCT"
   },
   "outputs": [],
   "source": [
    "### your code ###\n",
    "### your code ###\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
