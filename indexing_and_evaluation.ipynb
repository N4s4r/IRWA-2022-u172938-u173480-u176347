{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29168342-ae0b-4480-a3d1-a7d589cc2a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e4eeb3-8312-4cf6-bdbc-f66ae9e808f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f80c5-23a4-4a99-802c-572cf6babdc4",
   "metadata": {},
   "source": [
    "# Building inverted index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3907ca5-f6d2-424a-a692-dad8e56147cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_vocabulary(tweet, docId):\n",
    "    return {term: docId for term in tweet.split(' ')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3d1fc8-0475-440d-91de-8abe308b1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(dicts):\n",
    "    vocab = defaultdict(list)\n",
    "    for dic in dicts:\n",
    "        for term in dic:\n",
    "            vocab[term].append(dic[term])\n",
    "    return dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd7321e-e1b9-4b52-b4d0-4db4906cef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 10671 words\n"
     ]
    }
   ],
   "source": [
    "tweets_dicts = map(extract_tweet_vocabulary, df['Tweet'], df['DocID'])\n",
    "vocabulary = merge_dicts(tweets_dicts)\n",
    "print(f\"Vocabulary has {len(vocabulary)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd674950-950f-4724-aa08-7578250e7478",
   "metadata": {},
   "source": [
    "## Propose test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cfd8d3-dcd8-423d-a8c4-39a41acab84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TF_IDF(df, vocabulary):\n",
    "    terms = vocabulary.keys()\n",
    "    docs = df.DocID\n",
    "    N = len(docs)\n",
    "    tf_idf = dict()\n",
    "    \n",
    "    for doc in docs:\n",
    "        tf_idf[doc] = {}\n",
    "    \n",
    "    for term in terms:\n",
    "        for doc in vocabulary[term]:\n",
    "            tf = df[df.DocID == doc].Tweet.iloc[0].split().count(term)\n",
    "            if tf>0:\n",
    "                df_i = len(vocabulary[term])\n",
    "                tf_idf[doc][term] = (1+np.log(tf))*np.log(N/df_i)\n",
    "            else:\n",
    "                tf_idf[doc][term] = 0\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831d8884-16c6-41c3-a01f-8a207686c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc2norm(tf_idf):\n",
    "    docs = tf_idf.keys()\n",
    "    doc2norm = {}\n",
    "    for doc in docs:\n",
    "        doc2norm[doc] = np.linalg.norm(np.array(list(tf_idf[doc].values())))\n",
    "    return doc2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2554268-a7d0-402a-8b0a-d1bbcae3ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_term2sum(tf_idf):\n",
    "    terms = vocabulary.keys()\n",
    "    docs = tf_idf.keys()\n",
    "    term2sum = {term:0 for term in terms}\n",
    "    for doc in docs:\n",
    "        for term, value in tf_idf[doc].items():\n",
    "            term2sum[term] += value\n",
    "    term2sum = {t: v for t, v in sorted(term2sum.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return term2sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2278a136-c1b8-4f5e-8a4d-52696ab3d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = build_TF_IDF(df, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bfb80eb-b772-4eac-be9d-58a1fdab44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2norm = find_doc2norm(tf_idf)\n",
    "term2sum = find_term2sum(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e656439-e5bd-4523-aea4-26a043b19dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 terms:\n",
      "\t1431.550904893792 -> florida \n",
      "\t1383.119940611485 -> hurrican \n",
      "\t1353.611241498308 -> ian \n",
      "\t981.8070760038978 -> amp \n",
      "\t962.6318986223986 -> help \n",
      "\t878.8508807261255 -> storm \n",
      "\t794.7542557615245 -> flood \n",
      "\t779.361058496029 -> power \n",
      "\t763.8723112670441 -> carolina \n",
      "\t721.5594989731698 -> go \n",
      "\t719.9749917232326 -> south \n",
      "\t717.5405051743975 -> peopl \n",
      "\t703.8804288243131 -> make \n",
      "\t697.07497650128 -> wind \n",
      "\t692.1225245976001 -> landfal \n",
      "\t673.1614057906999 -> damag \n",
      "\t642.3890813702398 -> impact \n",
      "\t641.8769650785212 -> safe \n",
      "\t641.0670835188777 -> get \n",
      "\t637.6271428214658 -> u \n"
     ]
    }
   ],
   "source": [
    "top = 20\n",
    "print(f\"Top {top} terms:\")\n",
    "for i in range(top):\n",
    "    term, value = list(term2sum.items())[i]\n",
    "    print(f\"\\t{value} -> {term} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226d425-2cf8-441b-a510-6867ce80047a",
   "metadata": {},
   "source": [
    "From these results whe have concluded that some queries could be:\n",
    "\n",
    "Q1=\"florida hurrican\"\n",
    "\n",
    "Q2=\"help people in florida\"\n",
    "\n",
    "Q3=\"hurrican ian major damages\"\n",
    "\n",
    "Q4=\"storm impact in Florida\"\n",
    "\n",
    "Q5=\"floodings in the south\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a1119-de22-44ab-a6f9-e566440bced0",
   "metadata": {},
   "source": [
    "## Rank your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc9692c-3923-48fd-9f01-2c4e6be23632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet content removing stop words, contractionas and urls\n",
    "    lemmatizing and stemming words to keep a single word for each family of words\n",
    "    transforming in lowercase, removing special characters [#, @, .] \n",
    "    (since it is included in another column on the dataframe)\n",
    "    \n",
    "    return tokenized tweet (list of words after applying the previous steps).\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (tweet) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    ## START CODE\n",
    "    line = line.lower() ##Transform in lowercase\n",
    "    line = re.sub(r\"[^A-Za-z 0-9 ']+\", '', line) # remove emojis and any other special character\n",
    "    stop_words = set(stopwords.words(\"english\")) # removing stopwords\n",
    "    line = ' '.join([contractions.fix(x) for x in line.split(' ')]) # expaning verb abreviations: i'll -> i will \n",
    "    line = re.sub(\"'\", '', line) \n",
    "    line = line.split(' ')\n",
    "    line = [x for x in line if x and x not in stop_words]\n",
    "    line = filter(lambda x:x[0:5]!='https', line) # removing links\n",
    "    line = [x for x in line]\n",
    "    ps = PorterStemmer() \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    line = [lemmatizer.lemmatize(x) for x in line] # keeping the singular form of each noun: feet --> foot\n",
    "    line = [ps.stem(x) for x in line] # keeping the root of each family of words: dancer --> danc\n",
    "    \n",
    "    ## END CODE\n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db949f6a-7716-4e78-bf88-fac8ddd58c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query, tf_idf, vocabulary):\n",
    "    N = len(tf_idf.keys())\n",
    "    terms_query = build_terms(query).split()\n",
    "    terms_q = list(set(terms_query))\n",
    "    tf_idf_q = dict()\n",
    "    \n",
    "    for term_q in terms_q:\n",
    "        f_iq = terms_query.count(term_q)\n",
    "        if term_q not in vocabulary:\n",
    "            continue\n",
    "        df_i = len([doc for doc in vocabulary[term_q] if doc in tf_idf.keys()])\n",
    "        tf_idf_q[term_q] = (1+np.log(f_iq))*np.log(N/df_i)\n",
    "    q_norm = np.linalg.norm(np.array(list(tf_idf_q.values())))\n",
    "    doc2score = dict()\n",
    "    \n",
    "    doc2norm = find_doc2norm(tf_idf)\n",
    "    \n",
    "    for doc, dix in tf_idf.items():\n",
    "        dot_product = 0\n",
    "        for term, value in dix.items():\n",
    "            if term in tf_idf_q.keys():\n",
    "                dot_product += value * tf_idf_q[term]\n",
    "        doc2score[doc] = dot_product / (doc2norm[doc] * q_norm)\n",
    "    doc2score = {t: v for t, v in sorted(doc2score.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db5c176-4006-4dac-b47a-c5ce0c049e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1=\"florida hurrican\"\n",
    "Q2=\"help people in florida\"\n",
    "Q3=\"hurrican ian major damages\"\n",
    "Q4=\"storm impact in Florida\"\n",
    "Q5=\"floodings in the south\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f09fab-22f8-4d9a-9763-69ac27393962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 docs for query 1:\n",
      "\tdoc_640 -> 0.5015566349200697 -> hurrican ian hurricaneian\n",
      "\tdoc_733 -> 0.32852179583698915 -> experi hurrican check hurricaneian bless hurrican florida\n",
      "\tdoc_1071 -> 0.29589174718605576 -> hope peopl florida safe n sound hurrican ian hurricaneian florida\n",
      "\tdoc_2877 -> 0.2574191208525311 -> hurrican ian cuba florida south carolinahurricaneian\n",
      "\tdoc_1993 -> 0.25623464377621896 -> help affect hurrican ian florida via usatoday hurricaneian\n",
      "\tdoc_2051 -> 0.25623464377621896 -> help affect hurrican ian florida via usatoday hurricaneian\n",
      "\tdoc_2108 -> 0.25623464377621896 -> help affect hurrican ian florida via usatoday hurricaneian\n",
      "\tdoc_2183 -> 0.25623464377621896 -> help affect hurrican ian florida via usatoday hurricaneian\n",
      "\tdoc_2637 -> 0.24240947964633366 -> repost gma imag show hurrican ian destruct make landfal florida west coast categori 4 hurrican hurricaneian hurrican weather florida news\n",
      "\tdoc_3848 -> 0.23601494687859806 -> care florida hurricaneian\n"
     ]
    }
   ],
   "source": [
    "doc2score_Q1 = rank(Q1, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1:\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q1.items())[i]\n",
    "    print(f\"\\t{doc} -> {score} -> {df[df.DocID == doc].Tweet.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56182829-0bfd-40fa-9484-14774bad959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 docs for query 2:\n",
      "\tdoc_3682 -> 0.37023387134694796 -> good morn love yesterday employ sent email donat help florida hurricaneian victim want hear peopl help theme park ok concern peopl peopl help\n",
      "\tdoc_812 -> 0.34574116813941735 -> look organ provid help hurricaneian victim central florida tri connect group peopl need help know group peopl contact\n",
      "\tdoc_2817 -> 0.3252392212025381 -> hurricaneian impact million peopl name florida cuba organ donat support help need\n",
      "\tdoc_477 -> 0.32331316732387927 -> help way hurricaneian\n",
      "\tdoc_898 -> 0.30369425198511185 -> donaldjtrumpjr even tri open chariti help peopl florida recov hurricaneian\n",
      "\tdoc_1071 -> 0.2999415370477475 -> hope peopl florida safe n sound hurrican ian hurricaneian florida\n",
      "\tdoc_584 -> 0.2983730578974018 -> join fox support redcross relief effort help peopl affect hurricaneian\n",
      "\tdoc_2726 -> 0.2983730578974018 -> join fox support redcross relief effort help peopl affect hurricaneian\n",
      "\tdoc_2027 -> 0.2810946956282909 -> keycnewsnow thank share red cross help affect hurricaneian rais awar peopl help\n",
      "\tdoc_3560 -> 0.27830971153727796 -> join ml redcross help peopl affect hurricaneian donat help emerg prepared respons recoveri disast\n"
     ]
    }
   ],
   "source": [
    "doc2score_Q2 = rank(Q2, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 2:\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q2.items())[i]\n",
    "    print(f\"\\t{doc} -> {score} -> {df[df.DocID == doc].Tweet.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "225808cf-cf0c-4491-a14f-900d7a146a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 docs for query 3:\n",
      "\tdoc_1083 -> 0.3880462691741326 -> flood major concern gardenc hurricaneian\n",
      "\tdoc_640 -> 0.3843417729584876 -> hurrican ian hurricaneian\n",
      "\tdoc_3288 -> 0.2689334506116603 -> still without power amp estim restor went hotel minor damag roof amp flood damag noth major everyth fridg amp freezer gone could much wors hurricaneian\n",
      "\tdoc_2660 -> 0.25304353476848773 -> aftermath major storm awar potenti down power line hazard hurricaneian staysaf\n",
      "\tdoc_1494 -> 0.23537731979961168 -> holi cow major flood kissimme via wftv hurricaneian 03\n",
      "\tdoc_647 -> 0.22401974912104272 -> interest point discuss go forward build code state law chang tri mitig damag loss life result increas frequenc major hurrican landfal along conu hurricaneian\n",
      "\tdoc_3752 -> 0.20912460871585292 -> iron hurricaneian hit democrat major miamidad broward counti govrondesanti would ask relief fund\n",
      "\tdoc_1561 -> 0.207687617650036 -> januari year ef2 tornado caus major damag iona southern fort myer flood like total everyth left tornado ian view tropicana 55 mobil home commun hurricaneian srhelic spann\n",
      "\tdoc_1404 -> 0.1997572636790669 -> 40 florida nurs home evacu hurrican ian wake known resid death thursday state mobil emerg resourc prevent major catastroph florida hurricaneian emerg nursinghom fhca\n",
      "\tdoc_2845 -> 0.19934228915044366 -> victim hurricaneian thought climatechang becom sever frequenc amp intens major weather event visit full climat chang campaign\n"
     ]
    }
   ],
   "source": [
    "doc2score_Q3 = rank(Q3, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 3:\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q3.items())[i]\n",
    "    print(f\"\\t{doc} -> {score} -> {df[df.DocID == doc].Tweet.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d49a30b0-0a6d-4750-8fb9-db374b12db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 docs for query 4:\n",
      "\tdoc_2045 -> 0.4365974215379949 -> hurrican ian left devast impact mani part florida join u keep impact storm thought\n",
      "\tdoc_1065 -> 0.353869704308813 -> thought neighbor florida impact devast hurricaneian\n",
      "\tdoc_3406 -> 0.3418966093792788 -> thought prayer impact hurricaneian\n",
      "\tdoc_611 -> 0.31966865497209546 -> thought resid florida impact hurricaneian learn storm respons along tip affect sever weather\n",
      "\tdoc_1519 -> 0.3188829853498068 -> impact hurrican ian thought prayer hurricaneian\n",
      "\tdoc_1029 -> 0.30023694973777965 -> thought prayer everyon impact hurricaneian\n",
      "\tdoc_3956 -> 0.29640674751967616 -> great way help impact hurricaneian\n",
      "\tdoc_965 -> 0.27310342211111505 -> donat fund fellow american florida impact hurricaneian go\n",
      "\tdoc_446 -> 0.26903046992799906 -> pawley island polic keep tab hurricaneian storm impact area\n",
      "\tdoc_3475 -> 0.26667223300762155 -> yesterday storm hurricaneian\n"
     ]
    }
   ],
   "source": [
    "doc2score_Q4 = rank(Q4, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 4:\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q4.items())[i]\n",
    "    print(f\"\\t{doc} -> {score} -> {df[df.DocID == doc].Tweet.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e649f8c-a020-434f-83e7-5e8a75e9bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 docs for query 5:\n",
      "\tdoc_254 -> 0.5127787107942561 -> south carolina hurricaneian\n",
      "\tdoc_174 -> 0.4146748622439571 -> south carolina hurricaneian go\n",
      "\tdoc_493 -> 0.40233752258738564 -> south myrtl beach south carolina hurricaneian ian scwx\n",
      "\tdoc_249 -> 0.35646005431101074 -> hurricaneian make landfal south carolina\n",
      "\tdoc_2505 -> 0.35646005431101074 -> hurricaneian make landfal south carolina\n",
      "\tdoc_1289 -> 0.3368443875337799 -> flood garden citi south carolina gardenc southcarolina hurricaneian\n",
      "\tdoc_2484 -> 0.3065910954392718 -> flood trap mani florida ian head south carolinahttpstcozmufhc9egm hurricaneian\n",
      "\tdoc_2545 -> 0.29047616402264914 -> busi open south florida hurricaneian\n",
      "\tdoc_1435 -> 0.26770369849230224 -> charleston south carolina flood mani road around citi impass rain expect hurricaneian charleston southcarolina scwx\n",
      "\tdoc_2204 -> 0.26573923268110056 -> get latest hurricaneian near south carolina\n"
     ]
    }
   ],
   "source": [
    "doc2score_Q5 = rank(Q5, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 5:\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q5.items())[i]\n",
    "    print(f\"\\t{doc} -> {score} -> {df[df.DocID == doc].Tweet.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ceebb-9bd6-4324-873d-4d2f69a6704a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "944f18dc-d5c7-4c07-b60f-7d6b6f31b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation = pd.read_csv('data/evaluation_gt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d50a863-d33f-46fe-8955-2ffafc4c0f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0ca29f2-aa2b-46da-a0ce-4fb21b58f07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc  query_id  label\n",
       "0   doc_12         1      1\n",
       "1    doc_9         1      1\n",
       "2   doc_18         1      1\n",
       "3   doc_45         1      1\n",
       "4  doc_501         1      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cfdc034-19e9-4cae-a6d5-f5ff70146945",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_Q1 = \"Landfall in South Carolina\"\n",
    "E_Q2 = \"Help and recovery during the hurricane disaster\"\n",
    "E_Q3 = \"Floodings in South Carolina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb9bb0c5-5f76-460f-893a-52acd798995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_q1 = df_evaluation[df_evaluation.query_id == 1]\n",
    "evaluation_q2 = df_evaluation[df_evaluation.query_id == 2]\n",
    "evaluation_q3 = df_evaluation[df_evaluation.query_id == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b3d2085-ad54-4168-af58-e98d64c6dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_evaluation(query, evaluation, K, ranking):\n",
    "    print(\"Prior Evaluation for query:\", query)\n",
    "    docID_retrieved = [x[0] for x in ranking[0:K]]\n",
    "    scores = [x[1] for x in ranking[0:K]]\n",
    "    \n",
    "    docs_retieved = evaluation[evaluation.doc.isin(docID_retrieved)]\n",
    "    num_rel_docs_retrieved = docs_retieved.label.sum()\n",
    "    precision = num_rel_docs_retrieved/K\n",
    "    print(\"Precision@\"+str(K)+\":\", precision)\n",
    "    \n",
    "    num_relevant_docs = len(evaluation[evaluation.label==1])\n",
    "    recall = num_rel_docs_retrieved/num_relevant_docs\n",
    "    print(\"Recall@\"+str(K)+\":\", recall)\n",
    "    \n",
    "    avg_precision = 0\n",
    "    docs_relevant = 0\n",
    "    rr = 0\n",
    "    for i in range(K):\n",
    "        if docs_retieved[docs_retieved.doc == docID_retrieved[i]].label.iloc[0]==1:\n",
    "            if docs_relevant == 0:\n",
    "                rr = 1/(i+1)\n",
    "            docs_relevant+=1\n",
    "            avg_precision+=docs_relevant/(i+1)\n",
    "    avg_precision = avg_precision/num_rel_docs_retrieved\n",
    "    print(\"Average Precision@\"+str(K)+\":\", avg_precision)\n",
    "    \n",
    "    F_1 = 2*(1/(1/precision+1/recall))\n",
    "    print(\"F1-Score:\", F_1)\n",
    "    \n",
    "    actual = 0\n",
    "    ideal = 0\n",
    "    for i in range(K):\n",
    "        rel = docs_retieved[docs_retieved.doc == docID_retrieved[i]].label.iloc[0]\n",
    "        actual += (2**rel-1)/np.log(2+i)\n",
    "        ideal += 1/np.log(2+1)\n",
    "    ndcg = actual/ideal\n",
    "    print(\"Normalized Discounted Cumulative Gain (NDCG):\", ndcg)\n",
    "    \n",
    "    return precision, recall, avg_precision, rr, F_1, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75365a99-8024-4782-b2d0-bd0de529586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(avg_p_1, avg_p_2, avg_p_3, rr_1, rr_2, rr_3):\n",
    "    m_a_p = (avg_p_1+avg_p_2+avg_p_3)/3\n",
    "    print(\"Mean Average Precision (MAP):\", m_a_p)\n",
    "    \n",
    "    mrr=(rr_1+rr_2+rr_3)/3\n",
    "    print(\"Mean Reciprocal Rank (MRR):\", mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22ff9100-a846-47da-878a-6d6656dd0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_Q1 = {k: tf_idf[k] for k in set(evaluation_q1.doc)}\n",
    "tf_idf_Q2 = {k: tf_idf[k] for k in set(evaluation_q2.doc)}\n",
    "tf_idf_Q3 = {k: tf_idf[k] for k in set(evaluation_q3.doc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2b100bb-7b70-499d-9b8a-404d9abc1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_E_Q1 = list(rank(E_Q1, tf_idf_Q1, vocabulary).items())\n",
    "ranking_E_Q2 = list(rank(E_Q2, tf_idf_Q2, vocabulary).items())\n",
    "ranking_E_Q3 = list(rank(E_Q3, tf_idf_Q3, vocabulary).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b11648b-e150-47fc-9631-7953b20fa8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Evaluation for query: Landfall in South Carolina\n",
      "Precision@10: 1.0\n",
      "Recall@10: 1.0\n",
      "Average Precision@10: 1.0\n",
      "F1-Score: 1.0\n",
      "Normalized Discounted Cumulative Gain (NDCG): 0.7201371170671467\n"
     ]
    }
   ],
   "source": [
    "p1, r1, avg_p_1, rr_1, F1_1, ndcg_1 = prior_evaluation(E_Q1, evaluation_q1, top, ranking_E_Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c4cd63a-a8e4-4836-8e6d-8a1e61d96236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Evaluation for query: Help and recovery during the hurricane disaster\n",
      "Precision@10: 0.8\n",
      "Recall@10: 0.8\n",
      "Average Precision@10: 0.9085317460317459\n",
      "F1-Score: 0.8\n",
      "Normalized Discounted Cumulative Gain (NDCG): 0.6088223977906009\n"
     ]
    }
   ],
   "source": [
    "p2, r2, avg_p_2, rr_2, F1_2, ndcg_2 = prior_evaluation(E_Q2, evaluation_q2, top, ranking_E_Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68775366-009f-423a-8233-7267522eb789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Evaluation for query: Floodings in South Carolina\n",
      "Precision@10: 0.9\n",
      "Recall@10: 0.9\n",
      "Average Precision@10: 0.7777777777777778\n",
      "F1-Score: 0.8999999999999999\n",
      "Normalized Discounted Cumulative Gain (NDCG): 0.5766093005960478\n"
     ]
    }
   ],
   "source": [
    "p3, r3, avg_p_3, rr_3, F1_3, ndcg_3 = prior_evaluation(E_Q3, evaluation_q3, top, ranking_E_Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1180b5f-7e05-444b-afa6-50fc197d14ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.895436507936508\n",
      "Mean Reciprocal Rank (MRR): 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_system(avg_p_1, avg_p_2, avg_p_3, rr_1, rr_2, rr_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ba909-7372-42f0-8215-f306586c62d4",
   "metadata": {},
   "source": [
    "# 2-dimensional representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b536bb-70db-4ed6-a291-21105b504bc7",
   "metadata": {},
   "source": [
    "Choose one vector representation, TF-IDF or word2vec, and represent the tweets in a\n",
    "two-dimensional scatter plot through the T-SNE (T-distributed Stochastic Neighbor\n",
    "Embedding) algorithm. To do so, you may need first to represent the word as a\n",
    "vector, and then the tweet, i.e., resulted as the average value over the words\n",
    "involved. Any other option rather than T-SNE may be used, but needs to be justified.\n",
    "HINT: You don’t have to know all the theoretical details used in T-SNE, just use the\n",
    "proper library and generate the output and play with it.\n",
    "Also, you can choose to perform an alternative method to generate a 2-dimensional\n",
    "representation for the word embeddings (like PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5eb6908-1435-4523-90b4-b9e63d8724ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.DataFrame.from_dict(tf_idf_Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "521a09a7-8667-425c-a700-3a27a22f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "805d953b-bb79-46c9-964d-592c3bed091d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x282ad944640>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([list(vectors[t]) for t in tf_idf_Q1.keys()], dtype=object)\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(X)\n",
    "x_ = [p[0] for p in X_embedded]\n",
    "y_ = [p[1] for p in X_embedded]\n",
    "matplotlib.pyplot.scatter(x_, y_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
