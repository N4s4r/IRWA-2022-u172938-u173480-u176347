{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10d28d4-95c2-4828-a8b3-77312ec1a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34761a12-bd1e-4af1-9924-dc4b69c37f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54dd25e-2675-45af-9e95-5a78b1ec5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet documents in the corpus: 4000\n"
     ]
    }
   ],
   "source": [
    "docs_path = 'data/tweet_document_ids_map.csv'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "lines = [l.strip().replace(' +', ' ') for l in lines]\n",
    "print(\"Total number of tweet documents in the corpus: {}\".format(len(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "46a6f513-a3d2-4755-be39-c8b17e30d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets in the corpus: 4001\n"
     ]
    }
   ],
   "source": [
    "tweets_path = 'data/tw_hurricane_data.json'\n",
    "with open(tweets_path) as fp:\n",
    "    tweets = fp.readlines()\n",
    "tweets = [t for t in tweets]\n",
    "print(\"Total number of tweets in the corpus: {}\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9e4ffdec-c64d-4eb6-b373-2c9827e54741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tweets[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "87ce1c56-f2e8-4193-a923-59665f30304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_json=[]\n",
    "i=0\n",
    "for t in tweets:\n",
    "    if i != 4000:\n",
    "        tweets_json.append(json.loads(t)) #The element 4000 is an empty string, if we don't avoid it the code breaks\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4aceac2e-d736-4e48-ad8d-0b0e8ebb189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at                   id               id_str  \\\n",
      "0  Fri Sep 30 18:39:08 +0000 2022  1575918182698979328  1575918182698979328   \n",
      "1  Fri Sep 30 18:39:01 +0000 2022  1575918151862304768  1575918151862304768   \n",
      "2  Fri Sep 30 18:38:58 +0000 2022  1575918140839673873  1575918140839673873   \n",
      "3  Fri Sep 30 18:38:57 +0000 2022  1575918135009738752  1575918135009738752   \n",
      "4  Fri Sep 30 18:38:53 +0000 2022  1575918119251419136  1575918119251419136   \n",
      "\n",
      "                                           full_text  truncated  \\\n",
      "0  So this will keep spinning over us until 7 pmâ€¦...      False   \n",
      "1  Our hearts go out to all those affected by #Hu...      False   \n",
      "2  Kissimmee neighborhood off of Michigan Ave. \\n...      False   \n",
      "3  I have this one tree in my backyard that scare...      False   \n",
      "4  @AshleyRuizWx @Stephan89441722 @lilmizzheidi @...      False   \n",
      "\n",
      "  display_text_range                                           entities  \\\n",
      "0            [0, 76]  {'hashtags': [{'text': 'HurricaneIan', 'indice...   \n",
      "1           [0, 136]  {'hashtags': [{'text': 'HurricaneIan', 'indice...   \n",
      "2            [0, 58]  {'hashtags': [{'text': 'HurricaneIan', 'indice...   \n",
      "3           [0, 141]  {'hashtags': [{'text': 'scwx', 'indices': [122...   \n",
      "4         [127, 280]  {'hashtags': [{'text': 'HurricaneIan', 'indice...   \n",
      "\n",
      "                                   extended_entities  \\\n",
      "0  {'media': [{'id': 1575918178261254162, 'id_str...   \n",
      "1                                                NaN   \n",
      "2  {'media': [{'id': 1575918121080311808, 'id_str...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                            metadata  \\\n",
      "0  {'iso_language_code': 'en', 'result_type': 're...   \n",
      "1  {'iso_language_code': 'en', 'result_type': 're...   \n",
      "2  {'iso_language_code': 'en', 'result_type': 're...   \n",
      "3  {'iso_language_code': 'en', 'result_type': 're...   \n",
      "4  {'iso_language_code': 'en', 'result_type': 're...   \n",
      "\n",
      "                                              source  ...  is_quote_status  \\\n",
      "0  <a href=\"http://twitter.com/download/iphone\" r...  ...            False   \n",
      "1  <a href=\"https://sproutsocial.com\" rel=\"nofoll...  ...            False   \n",
      "2  <a href=\"http://twitter.com/download/iphone\" r...  ...            False   \n",
      "3  <a href=\"http://twitter.com/download/iphone\" r...  ...            False   \n",
      "4  <a href=\"http://twitter.com/download/iphone\" r...  ...            False   \n",
      "\n",
      "  retweet_count  favorite_count favorited retweeted possibly_sensitive lang  \\\n",
      "0             0               0     False     False              False   en   \n",
      "1             0               0     False     False                NaN   en   \n",
      "2             0               0     False     False              False   en   \n",
      "3             0               0     False     False                NaN   en   \n",
      "4             0               0     False     False                NaN   en   \n",
      "\n",
      "  quoted_status_id quoted_status_id_str quoted_status  \n",
      "0              NaN                  NaN           NaN  \n",
      "1              NaN                  NaN           NaN  \n",
      "2              NaN                  NaN           NaN  \n",
      "3              NaN                  NaN           NaN  \n",
      "4              NaN                  NaN           NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "dt_tweets = pd.DataFrame.from_dict(data=tweets_json)\n",
    "print(dt_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4443927e-be98-4280-84d6-75ea8fc9a529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'hashtags': [{'text': 'HurricaneIan', 'indice...\n",
       "Name: entities, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_tweets.head(1)['entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013d020-38d7-447e-8df7-18936af9599d",
   "metadata": {},
   "source": [
    "## HINT\n",
    "Take into account that for future queries, the final output must return (when\n",
    "present) the following information for each of the selected documents: Tweet |\n",
    "Username | Date | Hashtags | Likes | Retweets | Url (here the â€œUrlâ€ means the\n",
    "tweet link).\n",
    "\n",
    "## Marta: el url no he sapigut detectar quin era aixÃ­ q de moment ho he deixat buit... I els hastags no estic del tot segura el q volem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ec2fafee-4ce8-42c7-9e6d-be08d2089273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet              Username  \\\n",
      "0  This is awful.   https://t.co/BzhXaAPv7Bï¿¼\\n\\n#...      Ohemgeeitsalyssa   \n",
      "1  Damage in my area in Punta Gorda...a thread. I...             CJ Haddad   \n",
      "2  So it really wasn't #HurricaneIan that flooded...              @jganyfl   \n",
      "3  Why isnâ€™t @OsceolaCountyFl listed on the @fema...         BaconBitsNews   \n",
      "4  The CFRD, @CarrboroPD , Carrboro Public Works,...  Carrboro Fire-Rescue   \n",
      "\n",
      "                             Date      Hashtags Likes Retweets   Url  \n",
      "0  Fri Sep 30 14:32:56 +0000 2022      DeSantis     0        0  None  \n",
      "1  Fri Sep 30 14:32:56 +0000 2022  HurricaneIan     2        1  None  \n",
      "2  Fri Sep 30 14:32:57 +0000 2022  HurricaneIan    16        8  None  \n",
      "3  Fri Sep 30 14:33:01 +0000 2022     Kissimmee     0        0  None  \n",
      "4  Fri Sep 30 14:33:06 +0000 2022  CarrboroSafe     2        0  None  \n"
     ]
    }
   ],
   "source": [
    "tweets_query_format=pd.DataFrame(columns=['Tweet', 'Username', 'Date', 'Hashtags', 'Likes', 'Retweets', 'Url'])\n",
    "for index, row in dt_tweets.iterrows():\n",
    "    new_row = pd.DataFrame.from_dict({'Tweet': [row['full_text']], 'Username': [row['user']['name']], 'Date': [row['created_at']], 'Hashtags': [row['entities']['hashtags'][0]['text']], 'Likes': [row['favorite_count']], 'Retweets': [row['retweet_count']], 'Url': []}, orient='index').T\n",
    "    tweets_query_format=pd.concat([new_row, tweets_query_format]).reset_index(drop=True)\n",
    "print(tweets_query_format.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b3970aec-004d-4573-b418-7dd2d0b064c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is awful.   https://t.co/BzhXaAPv7Bï¿¼\\n\\n#...</td>\n",
       "      <td>Ohemgeeitsalyssa</td>\n",
       "      <td>Fri Sep 30 14:32:56 +0000 2022</td>\n",
       "      <td>DeSantis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Damage in my area in Punta Gorda...a thread. I...</td>\n",
       "      <td>CJ Haddad</td>\n",
       "      <td>Fri Sep 30 14:32:56 +0000 2022</td>\n",
       "      <td>HurricaneIan</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So it really wasn't #HurricaneIan that flooded...</td>\n",
       "      <td>@jganyfl</td>\n",
       "      <td>Fri Sep 30 14:32:57 +0000 2022</td>\n",
       "      <td>HurricaneIan</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why isnâ€™t @OsceolaCountyFl listed on the @fema...</td>\n",
       "      <td>BaconBitsNews</td>\n",
       "      <td>Fri Sep 30 14:33:01 +0000 2022</td>\n",
       "      <td>Kissimmee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The CFRD, @CarrboroPD , Carrboro Public Works,...</td>\n",
       "      <td>Carrboro Fire-Rescue</td>\n",
       "      <td>Fri Sep 30 14:33:06 +0000 2022</td>\n",
       "      <td>CarrboroSafe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>@AshleyRuizWx @Stephan89441722 @lilmizzheidi @...</td>\n",
       "      <td>Tess ðŸ’‹</td>\n",
       "      <td>Fri Sep 30 18:38:53 +0000 2022</td>\n",
       "      <td>HurricaneIan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>I have this one tree in my backyard that scare...</td>\n",
       "      <td>alex âœ¨</td>\n",
       "      <td>Fri Sep 30 18:38:57 +0000 2022</td>\n",
       "      <td>scwx</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Kissimmee neighborhood off of Michigan Ave. \\n...</td>\n",
       "      <td>Christopher Heath</td>\n",
       "      <td>Fri Sep 30 18:38:58 +0000 2022</td>\n",
       "      <td>HurricaneIan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Our hearts go out to all those affected by #Hu...</td>\n",
       "      <td>Lytx</td>\n",
       "      <td>Fri Sep 30 18:39:01 +0000 2022</td>\n",
       "      <td>HurricaneIan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>So this will keep spinning over us until 7 pmâ€¦...</td>\n",
       "      <td>SuzðŸ‘»</td>\n",
       "      <td>Fri Sep 30 18:39:08 +0000 2022</td>\n",
       "      <td>HurricaneIan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet              Username  \\\n",
       "0     This is awful.   https://t.co/BzhXaAPv7Bï¿¼\\n\\n#...      Ohemgeeitsalyssa   \n",
       "1     Damage in my area in Punta Gorda...a thread. I...             CJ Haddad   \n",
       "2     So it really wasn't #HurricaneIan that flooded...              @jganyfl   \n",
       "3     Why isnâ€™t @OsceolaCountyFl listed on the @fema...         BaconBitsNews   \n",
       "4     The CFRD, @CarrboroPD , Carrboro Public Works,...  Carrboro Fire-Rescue   \n",
       "...                                                 ...                   ...   \n",
       "3995  @AshleyRuizWx @Stephan89441722 @lilmizzheidi @...                Tess ðŸ’‹   \n",
       "3996  I have this one tree in my backyard that scare...                alex âœ¨   \n",
       "3997  Kissimmee neighborhood off of Michigan Ave. \\n...     Christopher Heath   \n",
       "3998  Our hearts go out to all those affected by #Hu...                  Lytx   \n",
       "3999  So this will keep spinning over us until 7 pmâ€¦...                  SuzðŸ‘»   \n",
       "\n",
       "                                Date      Hashtags Likes Retweets   Url  \n",
       "0     Fri Sep 30 14:32:56 +0000 2022      DeSantis     0        0  None  \n",
       "1     Fri Sep 30 14:32:56 +0000 2022  HurricaneIan     2        1  None  \n",
       "2     Fri Sep 30 14:32:57 +0000 2022  HurricaneIan    16        8  None  \n",
       "3     Fri Sep 30 14:33:01 +0000 2022     Kissimmee     0        0  None  \n",
       "4     Fri Sep 30 14:33:06 +0000 2022  CarrboroSafe     2        0  None  \n",
       "...                              ...           ...   ...      ...   ...  \n",
       "3995  Fri Sep 30 18:38:53 +0000 2022  HurricaneIan     0        0  None  \n",
       "3996  Fri Sep 30 18:38:57 +0000 2022          scwx     0        0  None  \n",
       "3997  Fri Sep 30 18:38:58 +0000 2022  HurricaneIan     0        0  None  \n",
       "3998  Fri Sep 30 18:39:01 +0000 2022  HurricaneIan     0        0  None  \n",
       "3999  Fri Sep 30 18:39:08 +0000 2022  HurricaneIan     0        0  None  \n",
       "\n",
       "[4000 rows x 7 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_query_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a2f0540b-6aae-432a-a03b-a78a5e641555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line= line.lower() ## Transform in lowercase\n",
    "    line=line.replace('@', '')\n",
    "    line=line.replace('#', '')\n",
    "    line=line.replace('.', '')\n",
    "    line= line.split() ## Tokenize the text to get a list of terms\n",
    "    line=[x for x in line if x not in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line = filter(lambda x:x[0:5]!='https', line)\n",
    "    line=[stemmer.stem(x) for x in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b86bc35f-c563-434c-9210-db7484962ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patata\n"
     ]
    }
   ],
   "source": [
    "ss= \"patata https://t.co/bzhxaapv7b\"\n",
    "print(\" \".join(filter(lambda x:x[0:5]!='https', ss.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1455bbd2-ec28-4df2-ba6d-adfa1d97e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet              Username  \\\n",
      "0  [aw, desanti, busi, scheme, traffick, asylum, ...      Ohemgeeitsalyssa   \n",
      "1  [damag, area, punta, gordaa, thread, tropic, g...             CJ Haddad   \n",
      "2  [realli, hurricaneian, flood, florida, magatea...              @jganyfl   \n",
      "3  [isnâ€™t, osceolacountyfl, list, fema, website?,...         BaconBitsNews   \n",
      "4  [cfrd,, carrboropd, ,, carrboro, public, works...  Carrboro Fire-Rescue   \n",
      "\n",
      "                             Date      Hashtags Likes Retweets   Url  \n",
      "0  Fri Sep 30 14:32:56 +0000 2022      DeSantis     0        0  None  \n",
      "1  Fri Sep 30 14:32:56 +0000 2022  HurricaneIan     2        1  None  \n",
      "2  Fri Sep 30 14:32:57 +0000 2022  HurricaneIan    16        8  None  \n",
      "3  Fri Sep 30 14:33:01 +0000 2022     Kissimmee     0        0  None  \n",
      "4  Fri Sep 30 14:33:06 +0000 2022  CarrboroSafe     2        0  None  \n"
     ]
    }
   ],
   "source": [
    "tweets_query_format_processed=pd.DataFrame(columns=['Tweet', 'Username', 'Date', 'Hashtags', 'Likes', 'Retweets', 'Url'])\n",
    "tweets_query_format_processed = tweets_query_format.copy()\n",
    "for index, row in tweets_query_format_processed.iterrows():\n",
    "    tweets_query_format_processed.iloc[index]['Tweet']=build_terms(row['Tweet'])\n",
    "print(tweets_query_format_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6885d0a4-2a03-4c93-bd48-8a605209c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'day', 'saw', 'cow', 'without', 'tie', 'dress', 'uniform']\n"
     ]
    }
   ],
   "source": [
    "print(build_terms(\"One day I saw a cow without a tie dressed with a uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bbe4134e-8bbe-4dcd-a350-f28bd8c0a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "    for line in lines:  # Remember, lines contain all documents: article-id | article-title | article-body\n",
    "        #line_arr = line.split(\"\\\")\n",
    "        #print(line.split(\"\\t\"))\n",
    "        line_arr=line.split(\"\\t\")[1]\n",
    "        page_id = int(line_arr)\n",
    "        terms = build_terms(''.join(line_arr[1:])) # page_title + page_text\n",
    "        title = line_arr[1]\n",
    "        title_index[page_id]=title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,3]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de1a512d-7489-401c-9d6f-cb9ff09659e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 3.61 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index, title_index = create_index(lines)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
