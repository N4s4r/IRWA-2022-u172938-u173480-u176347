{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10d28d4-95c2-4828-a8b3-77312ec1a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34761a12-bd1e-4af1-9924-dc4b69c37f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54dd25e-2675-45af-9e95-5a78b1ec5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweet documents in the corpus: 4000\n"
     ]
    }
   ],
   "source": [
    "docs_path = 'data/tweet_document_ids_map.csv'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "lines = [l.strip().replace(' +', ' ') for l in lines]\n",
    "print(\"Total number of tweet documents in the corpus: {}\".format(len(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2f0540b-6aae-432a-a03b-a78a5e641555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line= line.lower() ## Transform in lowercase\n",
    "    line= line.split() ## Tokenize the text to get a list of terms\n",
    "    line=[x for x in line if x not in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line=[stemmer.stem(x) for x in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6885d0a4-2a03-4c93-bd48-8a605209c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'day', 'saw', 'cow', 'without', 'tie', 'dress', 'uniform']\n"
     ]
    }
   ],
   "source": [
    "print(build_terms(\"One day I saw a cow without a tie dressed with a uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbe4134e-8bbe-4dcd-a350-f28bd8c0a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "    for line in lines:  # Remember, lines contain all documents: article-id | article-title | article-body\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = build_terms(''.join(line_arr[1:])) # page_title + page_text\n",
    "        title = line_arr[1]\n",
    "        title_index[page_id]=title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,3]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de1a512d-7489-401c-9d6f-cb9ff09659e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'doc_1\\t1575918182698979328'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 2\u001b[0m index, title_index \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time to create the index: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mround(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time, \u001b[38;5;241m2\u001b[39m)))\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(lines)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:  \u001b[38;5;66;03m# Remember, lines contain all documents: article-id | article-title | article-body\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     line_arr \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     page_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     terms \u001b[38;5;241m=\u001b[39m build_terms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(line_arr[\u001b[38;5;241m1\u001b[39m:])) \u001b[38;5;66;03m# page_title + page_text\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     title \u001b[38;5;241m=\u001b[39m line_arr[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'doc_1\\t1575918182698979328'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index, title_index = create_index(lines)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
