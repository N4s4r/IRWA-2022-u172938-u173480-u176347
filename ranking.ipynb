{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Campero/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Campero/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Campero/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_tweets.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From these results of part 2 we had concluded that some queries could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1=\"florida hurrican\"\n",
    "\n",
    "Q2=\"help people in florida\"\n",
    "\n",
    "Q3=\"hurrican ian major damages\"\n",
    "\n",
    "Q4=\"storm impact in Florida\"\n",
    "\n",
    "Q5=\"floodings in the south\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Scoring using TF-IDF + Cosine Similarity \n",
    "Classical scoring, we have also seen during the practical labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_vocabulary(tweet, docId):\n",
    "    return {term: docId for term in tweet.split(' ')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(dicts):\n",
    "    vocab = defaultdict(list)\n",
    "    for dic in dicts:\n",
    "        for term in dic:\n",
    "            vocab[term].append(dic[term])\n",
    "    return dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 10671 words\n"
     ]
    }
   ],
   "source": [
    "tweets_dicts = map(extract_tweet_vocabulary, df['Tweet'], df['DocID'])\n",
    "vocabulary = merge_dicts(tweets_dicts)\n",
    "print(f\"Vocabulary has {len(vocabulary)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TF_IDF(df, vocabulary):\n",
    "    terms = vocabulary.keys()\n",
    "    docs = df.DocID\n",
    "    N = len(docs)\n",
    "    tf_idf = dict()\n",
    "    \n",
    "    for doc in docs:\n",
    "        tf_idf[doc] = {}\n",
    "    \n",
    "    for term in terms:\n",
    "        for doc in vocabulary[term]:\n",
    "            tf = df[df.DocID == doc].Tweet.iloc[0].split().count(term)\n",
    "            if tf>0:\n",
    "                df_i = len(vocabulary[term])\n",
    "                tf_idf[doc][term] = (1+np.log(tf))*np.log(N/df_i)\n",
    "            else:\n",
    "                tf_idf[doc][term] = 0\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc2norm(tf_idf):\n",
    "    docs = tf_idf.keys()\n",
    "    doc2norm = {}\n",
    "    for doc in docs:\n",
    "        doc2norm[doc] = np.linalg.norm(np.array(list(tf_idf[doc].values())))\n",
    "    return doc2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_term2sum(tf_idf):\n",
    "    terms = vocabulary.keys()\n",
    "    docs = tf_idf.keys()\n",
    "    term2sum = {term:0 for term in terms}\n",
    "    for doc in docs:\n",
    "        for term, value in tf_idf[doc].items():\n",
    "            term2sum[term] += value\n",
    "    term2sum = {t: v for t, v in sorted(term2sum.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return term2sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = build_TF_IDF(df, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2norm = find_doc2norm(tf_idf)\n",
    "term2sum = find_term2sum(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet content removing stop words, contractionas and urls\n",
    "    lemmatizing and stemming words to keep a single word for each family of words\n",
    "    transforming in lowercase, removing special characters [#, @, .] \n",
    "    (since it is included in another column on the dataframe)\n",
    "    \n",
    "    return tokenized tweet (list of words after applying the previous steps).\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (tweet) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    ## START CODE\n",
    "    line = line.lower() ##Transform in lowercase\n",
    "    line = re.sub(r\"[^A-Za-z 0-9 ']+\", '', line) # remove emojis and any other special character\n",
    "    stop_words = set(stopwords.words(\"english\")) # removing stopwords\n",
    "    line = ' '.join([contractions.fix(x) for x in line.split(' ')]) # expaning verb abreviations: i'll -> i will \n",
    "    line = re.sub(\"'\", '', line) \n",
    "    line = line.split(' ')\n",
    "    line = [x for x in line if x and x not in stop_words]\n",
    "    line = filter(lambda x:x[0:5]!='https', line) # removing links\n",
    "    line = [x for x in line]\n",
    "    ps = PorterStemmer() \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    line = [lemmatizer.lemmatize(x) for x in line] # keeping the singular form of each noun: feet --> foot\n",
    "    line = [ps.stem(x) for x in line] # keeping the root of each family of words: dancer --> danc\n",
    "    \n",
    "    ## END CODE\n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query, tf_idf, vocabulary):\n",
    "    N = len(tf_idf.keys())\n",
    "    terms_query = build_terms(query).split()\n",
    "    terms_q = list(set(terms_query))\n",
    "    tf_idf_q = dict()\n",
    "    \n",
    "    for term_q in terms_q:\n",
    "        f_iq = terms_query.count(term_q)\n",
    "        if term_q not in vocabulary:\n",
    "            continue\n",
    "        df_i = len([doc for doc in vocabulary[term_q] if doc in tf_idf.keys()])\n",
    "        if df_i>0:\n",
    "            tf_idf_q[term_q] = (1+np.log(f_iq))*np.log(N/df_i)\n",
    "        else:\n",
    "            tf_idf_q[term_q]=0\n",
    "    q_norm = np.linalg.norm(np.array(list(tf_idf_q.values())))\n",
    "    doc2score = dict()\n",
    "    \n",
    "    doc2norm = find_doc2norm(tf_idf)\n",
    "    \n",
    "    for doc, dix in tf_idf.items():\n",
    "        dot_product = 0\n",
    "        for term, value in dix.items():\n",
    "            if term in tf_idf_q.keys():\n",
    "                dot_product += value * tf_idf_q[term]\n",
    "        doc2score[doc] = dot_product / (doc2norm[doc] * q_norm)\n",
    "    doc2score = {t: v for t, v in sorted(doc2score.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q1 = rank(Q1, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1 ('{Q1}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q1.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q2 = rank(Q2, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1 ('{Q2}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q2.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q3 = rank(Q3, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1 ('{Q3}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q3.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q4 = rank(Q4, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1 ('{Q4}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q4.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q5 = rank(Q5, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1 ('{Q5}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q5.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take into account that for future queries, the final output must return (when\n",
    "present) the following information for each of the selected documents: Tweet |\n",
    "Username | Date | Hashtags | Likes | Retweets | Url (here the “Url” means the\n",
    "tweet link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Own score + cosine similarity\n",
    "\n",
    "Here the task is to create a new score, and it’s up to you to create a new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2extra_score = dict()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    extra_score = (row[\"Likes\"] + row[\"Retweets\"]) / len(row[\"Hashtags\"].split(\",\"))\n",
    "    doc2extra_score[row[\"DocID\"]] = extra_score\n",
    "\n",
    "mu = np.mean(list(doc2extra_score.values()))\n",
    "doc2extra_score = {k: v / mu for k, v in doc2extra_score.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_rank(query, tf_idf, vocabulary, doc2extra_score):\n",
    "    N = len(tf_idf.keys())\n",
    "    terms_query = build_terms(query).split()\n",
    "    terms_q = list(set(terms_query))\n",
    "    tf_idf_q = dict()\n",
    "    \n",
    "    for term_q in terms_q:\n",
    "        f_iq = terms_query.count(term_q)\n",
    "        if term_q not in vocabulary:\n",
    "            continue\n",
    "        df_i = len([doc for doc in vocabulary[term_q] if doc in tf_idf.keys()])\n",
    "        if df_i>0:\n",
    "            tf_idf_q[term_q] = (1+np.log(f_iq))*np.log(N/df_i)\n",
    "        else:\n",
    "            tf_idf_q[term_q]=0\n",
    "    q_norm = np.linalg.norm(np.array(list(tf_idf_q.values())))\n",
    "    doc2score = dict()\n",
    "    \n",
    "    doc2norm = find_doc2norm(tf_idf)\n",
    "    \n",
    "    for doc, dix in tf_idf.items():\n",
    "        dot_product = 0\n",
    "        for term, value in dix.items():\n",
    "            if term in tf_idf_q.keys():\n",
    "                dot_product += value * tf_idf_q[term]\n",
    "        doc2score[doc] = dot_product / (doc2norm[doc] * q_norm) + doc2extra_score[doc]\n",
    "    doc2score = {t: v for t, v in sorted(doc2score.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q1 = new_rank(Q1, tf_idf, vocabulary, doc2extra_score)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1 ('{Q1}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q1.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q2 = new_rank(Q2, tf_idf, vocabulary, doc2extra_score)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 2 ('{Q2}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q2.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q3 = new_rank(Q3, tf_idf, vocabulary, doc2extra_score)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 3 ('{Q3}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q3.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q4 = new_rank(Q4, tf_idf, vocabulary, doc2extra_score)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 4 ('{Q4}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q4.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score_Q5 = new_rank(Q5, tf_idf, vocabulary, doc2extra_score)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 5 ('{Q5}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q5.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# c) BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "a = {k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query, vocabulary, k1, b, L_ave, doc_contents):\n",
    "    '''\n",
    "    vocabulary = inverted index, dictionary with keys = terms an values = list of doc_ids\n",
    "    k1 = value to regulate xx\n",
    "    b = value to regulate yy\n",
    "    L_ave = average length of docs in words\n",
    "    doc_contents = dictionary where keys = doc_ids and values = list of terms (after text processing)\n",
    "    '''\n",
    "    \n",
    "    RSV = dict()\n",
    "    N = len(doc_contents)\n",
    "    terms_q = list(set(build_terms(query).split()))\n",
    "    idf = dict()\n",
    "    \n",
    "    # calculate idf for each term in the query \n",
    "    for t in terms_q:\n",
    "        f_tq = terms_q.count(t)\n",
    "        if t not in vocabulary:\n",
    "            continue\n",
    "        df_t = len(vocabulary[t])\n",
    "        idf[t] = np.log(N/df_t)\n",
    "        \n",
    "    # calculate RSVd for each document  \n",
    "    for doc in doc_contents.keys():\n",
    "        RSV[doc] = 0\n",
    "        Ld = len(doc_contents[doc])\n",
    "        for t in idf.keys():\n",
    "            tf_t_d = doc_contents[doc].count(t)\n",
    "\n",
    "            second_term = ((k1+1)*tf_t_d) / (k1*((1-b)+b*(Ld/L_ave))+tf_t_d)\n",
    "            RSV[doc] += idf[t]*second_term\n",
    "            \n",
    "    return {k: v for k, v in sorted(RSV.items(), key=lambda item: item[1], reverse=True)}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 1\n",
    "b = 1\n",
    "L_ave = np.mean([len(x.split()) for x in df.Tweet])\n",
    "print(L_ave)\n",
    "dictionary_doc = df.copy().drop(columns=['Username', 'Date', 'Hashtags', 'Likes', 'Retweets', 'Url'])\n",
    "dictionary_doc = dictionary_doc.set_index('DocID').T.to_dict('list')\n",
    "dictionary_doc = {k: x[0].split() for k, x in dictionary_doc.items()}\n",
    "doc2score_Q5 = BM25(Q5, vocabulary, k1, b, L_ave, dictionary_doc)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query ('{Q5}'):\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q5.items())[i]\n",
    "    print(f\"\\t {i+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Word2vec + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing doc words in a list of lists \n",
    "docs_as_words = []\n",
    "for k in dictionary_doc.keys():\n",
    "    docs_as_words.append(dictionary_doc[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model for word2vec\n",
    "model1 = gensim.models.Word2Vec(docs_as_words, min_count = 1,\n",
    "                              vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert any text to a vector\n",
    "def text2vec(text):\n",
    "\n",
    "    #compute a vector for each word in the text  \n",
    "    vectors = []\n",
    "    for w in text:\n",
    "        vector = model1.wv[w]\n",
    "        vectors.append(vector)\n",
    "\n",
    "    # sum all vectors previously computed\n",
    "    doc_to_vec = np.zeros(len(vectors[0]))\n",
    "    for v in vectors: \n",
    "        doc_to_vec += v\n",
    "\n",
    "    # take average of previous sum\n",
    "    text_2_vec = doc_to_vec/len(content) \n",
    "        \n",
    "    return text_2_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all tweets to vectors\n",
    "tweets_to_vecs = {}\n",
    "for doc, content in dictionary_doc.items():\n",
    "    tweets_to_vecs[doc] = text2vec(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing queries\n",
    "queries = [Q1, Q2, Q3, Q4, Q5]\n",
    "processed_queries = [build_terms(q) for q in queries]\n",
    "\n",
    "# removing words that don't appear in our vocabulary \n",
    "clean_queries = [[w for w in q.split(' ') if w in vocabulary.keys()] for q in queries]\n",
    "\n",
    "# converting all queries to vectors\n",
    "queries_to_vecs = {}\n",
    "i=0\n",
    "for q in clean_queries: \n",
    "    queries_to_vecs[f'Q{i}'] = text2vec(q)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    cos_sim = dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cos similarities for each (query, doc) pair and sort them \n",
    "cos_similarities = {}\n",
    "for query, query_vec in queries_to_vecs.items():\n",
    "    tweet_queries_sim = {}\n",
    "    for tweet, tweet_vec in tweets_to_vecs.items():\n",
    "        tweet_queries_sim[tweet] = cosine_similarity(tweet_vec, query_vec)\n",
    "        \n",
    "    tweet_queries_sim = {k: v for k, v in sorted(tweet_queries_sim.items(), key=lambda item: item[1], reverse=True)}\n",
    "    cos_similarities[query] = tweet_queries_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_docs = {}\n",
    "for query, docs in cos_similarities.items(): \n",
    "    i=0\n",
    "    top_20_docs[query] = []\n",
    "    for doc in docs.keys():\n",
    "        if i<20:\n",
    "            top_20_docs[query].append(doc)\n",
    "            i+=1\n",
    "        else: \n",
    "            break \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_20_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 20\n",
    "\n",
    "i=0\n",
    "for q in queries:\n",
    "    print(f\"Top {top} docs for query ('{q}'):\")\n",
    "    j=0\n",
    "    for doc in top_20_docs[f'Q{i}']:\n",
    "        score = cos_similarities[f'Q{i}'][doc]\n",
    "        print(f\"\\t {j+1}) {doc} with a score of {score} and with the following information:\\n{df[df.DocID == doc].iloc[0]}\")\n",
    "        j+=1\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
