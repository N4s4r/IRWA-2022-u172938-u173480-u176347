{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Josep\n",
      "[nltk_data]     Alet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>keep spin u 7 pmgo away alreadi hurricaneian</td>\n",
       "      <td>suzjdean</td>\n",
       "      <td>Fri Sep 30 18:39:08 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/suzjdean/status/1575918182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>heart go affect hurricaneian wish everyon road...</td>\n",
       "      <td>lytx</td>\n",
       "      <td>Fri Sep 30 18:39:01 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/lytx/status/15759181518623...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>kissimme neighborhood michigan ave hurricaneian</td>\n",
       "      <td>CHeathWFTV</td>\n",
       "      <td>Fri Sep 30 18:38:58 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/CHeathWFTV/status/15759181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>one tree backyard scare poltergeist tree storm...</td>\n",
       "      <td>spiralgypsy</td>\n",
       "      <td>Fri Sep 30 18:38:57 +0000 2022</td>\n",
       "      <td>['scwx', 'HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/spiralgypsy/status/1575918...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>ashleyruizwx stephan89441722 lilmizzheidi mrsn...</td>\n",
       "      <td>Blondie610</td>\n",
       "      <td>Fri Sep 30 18:38:53 +0000 2022</td>\n",
       "      <td>['HurricaneIan']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Blondie610/status/15759181...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DocID                                              Tweet     Username  \\\n",
       "0  doc_1       keep spin u 7 pmgo away alreadi hurricaneian     suzjdean   \n",
       "1  doc_2  heart go affect hurricaneian wish everyon road...         lytx   \n",
       "2  doc_3    kissimme neighborhood michigan ave hurricaneian   CHeathWFTV   \n",
       "3  doc_4  one tree backyard scare poltergeist tree storm...  spiralgypsy   \n",
       "4  doc_5  ashleyruizwx stephan89441722 lilmizzheidi mrsn...   Blondie610   \n",
       "\n",
       "                             Date                  Hashtags  Likes  Retweets  \\\n",
       "0  Fri Sep 30 18:39:08 +0000 2022          ['HurricaneIan']      0         0   \n",
       "1  Fri Sep 30 18:39:01 +0000 2022          ['HurricaneIan']      0         0   \n",
       "2  Fri Sep 30 18:38:58 +0000 2022          ['HurricaneIan']      0         0   \n",
       "3  Fri Sep 30 18:38:57 +0000 2022  ['scwx', 'HurricaneIan']      0         0   \n",
       "4  Fri Sep 30 18:38:53 +0000 2022          ['HurricaneIan']      0         0   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://twitter.com/suzjdean/status/1575918182...  \n",
       "1  https://twitter.com/lytx/status/15759181518623...  \n",
       "2  https://twitter.com/CHeathWFTV/status/15759181...  \n",
       "3  https://twitter.com/spiralgypsy/status/1575918...  \n",
       "4  https://twitter.com/Blondie610/status/15759181...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_tweets.csv').drop(columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From these results of part 2 we had concluded that some queries could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1=\"florida hurrican\"\n",
    "\n",
    "Q2=\"help people in florida\"\n",
    "\n",
    "Q3=\"hurrican ian major damages\"\n",
    "\n",
    "Q4=\"storm impact in Florida\"\n",
    "\n",
    "Q5=\"floodings in the south\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Scoring using TF-IDF + Cosine Similarity \n",
    "Classical scoring, we have also seen during the practical labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_vocabulary(tweet, docId):\n",
    "    return {term: docId for term in tweet.split(' ')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(dicts):\n",
    "    vocab = defaultdict(list)\n",
    "    for dic in dicts:\n",
    "        for term in dic:\n",
    "            vocab[term].append(dic[term])\n",
    "    return dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 10671 words\n"
     ]
    }
   ],
   "source": [
    "tweets_dicts = map(extract_tweet_vocabulary, df['Tweet'], df['DocID'])\n",
    "vocabulary = merge_dicts(tweets_dicts)\n",
    "print(f\"Vocabulary has {len(vocabulary)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_TF_IDF(df, vocabulary):\n",
    "    terms = vocabulary.keys()\n",
    "    docs = df.DocID\n",
    "    N = len(docs)\n",
    "    tf_idf = dict()\n",
    "    \n",
    "    for doc in docs:\n",
    "        tf_idf[doc] = {}\n",
    "    \n",
    "    for term in terms:\n",
    "        for doc in vocabulary[term]:\n",
    "            tf = df[df.DocID == doc].Tweet.iloc[0].split().count(term)\n",
    "            if tf>0:\n",
    "                df_i = len(vocabulary[term])\n",
    "                tf_idf[doc][term] = (1+np.log(tf))*np.log(N/df_i)\n",
    "            else:\n",
    "                tf_idf[doc][term] = 0\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc2norm(tf_idf):\n",
    "    docs = tf_idf.keys()\n",
    "    doc2norm = {}\n",
    "    for doc in docs:\n",
    "        doc2norm[doc] = np.linalg.norm(np.array(list(tf_idf[doc].values())))\n",
    "    return doc2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_term2sum(tf_idf):\n",
    "    terms = vocabulary.keys()\n",
    "    docs = tf_idf.keys()\n",
    "    term2sum = {term:0 for term in terms}\n",
    "    for doc in docs:\n",
    "        for term, value in tf_idf[doc].items():\n",
    "            term2sum[term] += value\n",
    "    term2sum = {t: v for t, v in sorted(term2sum.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return term2sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = build_TF_IDF(df, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2norm = find_doc2norm(tf_idf)\n",
    "term2sum = find_term2sum(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet content removing stop words, contractionas and urls\n",
    "    lemmatizing and stemming words to keep a single word for each family of words\n",
    "    transforming in lowercase, removing special characters [#, @, .] \n",
    "    (since it is included in another column on the dataframe)\n",
    "    \n",
    "    return tokenized tweet (list of words after applying the previous steps).\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (tweet) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    ## START CODE\n",
    "    line = line.lower() ##Transform in lowercase\n",
    "    line = re.sub(r\"[^A-Za-z 0-9 ']+\", '', line) # remove emojis and any other special character\n",
    "    stop_words = set(stopwords.words(\"english\")) # removing stopwords\n",
    "    line = ' '.join([contractions.fix(x) for x in line.split(' ')]) # expaning verb abreviations: i'll -> i will \n",
    "    line = re.sub(\"'\", '', line) \n",
    "    line = line.split(' ')\n",
    "    line = [x for x in line if x and x not in stop_words]\n",
    "    line = filter(lambda x:x[0:5]!='https', line) # removing links\n",
    "    line = [x for x in line]\n",
    "    ps = PorterStemmer() \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    line = [lemmatizer.lemmatize(x) for x in line] # keeping the singular form of each noun: feet --> foot\n",
    "    line = [ps.stem(x) for x in line] # keeping the root of each family of words: dancer --> danc\n",
    "    \n",
    "    ## END CODE\n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query, tf_idf, vocabulary):\n",
    "    N = len(tf_idf.keys())\n",
    "    terms_query = build_terms(query).split()\n",
    "    terms_q = list(set(terms_query))\n",
    "    tf_idf_q = dict()\n",
    "    \n",
    "    for term_q in terms_q:\n",
    "        f_iq = terms_query.count(term_q)\n",
    "        if term_q not in vocabulary:\n",
    "            continue\n",
    "        df_i = len([doc for doc in vocabulary[term_q] if doc in tf_idf.keys()])\n",
    "        if df_i>0:\n",
    "            tf_idf_q[term_q] = (1+np.log(f_iq))*np.log(N/df_i)\n",
    "        else:\n",
    "            tf_idf_q[term_q]=0\n",
    "    q_norm = np.linalg.norm(np.array(list(tf_idf_q.values())))\n",
    "    doc2score = dict()\n",
    "    \n",
    "    doc2norm = find_doc2norm(tf_idf)\n",
    "    \n",
    "    for doc, dix in tf_idf.items():\n",
    "        dot_product = 0\n",
    "        for term, value in dix.items():\n",
    "            if term in tf_idf_q.keys():\n",
    "                dot_product += value * tf_idf_q[term]\n",
    "        doc2score[doc] = dot_product / (doc2norm[doc] * q_norm)\n",
    "    doc2score = {t: v for t, v in sorted(doc2score.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc2score_Q1 \u001b[38;5;241m=\u001b[39m rank(Q1, \u001b[43mtf_idf\u001b[49m, vocabulary)\n\u001b[0;32m      2\u001b[0m top \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m docs for query 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "doc2score_Q1 = rank(Q1, tf_idf, vocabulary)\n",
    "top = 10\n",
    "print(f\"Top {top} docs for query 1:\")\n",
    "for i in range(top):\n",
    "    doc, score = list(doc2score_Q1.items())[i]\n",
    "    print(f\"\\t{doc} -> {score} -> {df[df.DocID == doc].Tweet.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Own score + cosine similarity\n",
    "\n",
    "Here the task is to create a new score, and itâ€™s up to you to create a new one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 4, 4: 3, 1: 2, 2: 1, 0: 0}\n"
     ]
    }
   ],
   "source": [
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "a = {k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query, vocabulary, k1, b, L_ave, doc_contents):\n",
    "    '''\n",
    "    vocabulary = inverted index, dictionary with keys = terms an values = list of doc_ids\n",
    "    k1 = value to regulate xx\n",
    "    b = value to regulate yy\n",
    "    L_ave = average length of docs in words\n",
    "    doc_contents = dictionary where keys = doc_ids and values = list of terms (after text processing)\n",
    "    '''\n",
    "    \n",
    "    RSV = dict()\n",
    "    N = len(doc_contents)\n",
    "    terms_q = list(set(build_terms(query).split()))\n",
    "    idf = dict()\n",
    "    \n",
    "    # calculate idf for each term in the query \n",
    "    for t in terms_q:\n",
    "        f_tq = terms_query.count(t)\n",
    "        if t not in vocabulary:\n",
    "            continue\n",
    "        df_t = len(vocabulary[t])\n",
    "        idf[t] = np.log(N/df_t)\n",
    "        \n",
    "    # calculate RSVd for each document  \n",
    "    for doc in doc_contents.keys():\n",
    "        RSV[doc] = 0\n",
    "        Ld = len(doc_contents[doc])\n",
    "        for t in idf.keys():\n",
    "            tf_t_d = doc_contents[doc].count(t)\n",
    "\n",
    "            second_term = ((k1+1)*tf_t_d) / (k1*((1-b)+b*(Ld/L_ave))+tf_t_d)\n",
    "            RSV[doc] += idf[t]*second_term\n",
    "            \n",
    "    return {k: v for k, v in sorted(RSV.items(), key=lambda item: item[1], reverse=True)}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
